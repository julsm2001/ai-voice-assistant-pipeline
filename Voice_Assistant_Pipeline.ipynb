{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéôÔ∏è Cloud-Based Voice Assistant: STT-LLM-TTS Pipeline\n",
        "\n",
        "**Author:** Juli√°n Machuca Ram√≠rez\n",
        "**Date:** December 2025\n",
        "\n",
        "## Project Overview\n",
        "This notebook implements an **End-to-End Voice Interaction System** running entirely on the cloud using Google Colab. The pipeline integrates three distinct AI technologies to simulate a natural conversation loop:\n",
        "\n",
        "1.  **ASR (Automatic Speech Recognition):** `OpenAI Whisper` for high-fidelity audio transcription.\n",
        "2.  **LLM (Large Language Model):** `Llama 3.3 (70B)` via **Groq API** for low-latency inference and natural language understanding.\n",
        "3.  **TTS (Text-to-Speech):** `gTTS` for audio synthesis and response delivery.\n",
        "\n",
        "## Architecture\n",
        "`[User Audio Input] -> [Whisper Base Model] -> [Text Prompt] -> [Llama 3] -> [AI Response] -> [Audio Output]`"
      ],
      "metadata": {
        "id": "z6hRWdLz3zVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup\n",
        "Installing dependencies for Audio I/O, ASR, and LLM inference."
      ],
      "metadata": {
        "id": "upUC8beR4wnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ENVIRONMENT SETUP ---\n",
        "# Installing dependencies for Audio I/O, ASR, and LLM inference.\n",
        "# Using 'capture' to suppress verbose installation logs for a cleaner output.\n",
        "\n",
        "from IPython.utils import io\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"‚öôÔ∏è Setting up environment...\")\n",
        "\n",
        "with io.capture_output() as captured:\n",
        "    # System & Audio processing\n",
        "    !pip install ffmpeg-python pydub gTTS -q\n",
        "\n",
        "    # ASR: OpenAI Whisper\n",
        "    !pip install git+https://github.com/openai/whisper.git -q\n",
        "\n",
        "    # LLM: Groq API Client\n",
        "    !pip install groq -q\n",
        "\n",
        "# Imports\n",
        "import whisper\n",
        "import ffmpeg\n",
        "import numpy as np\n",
        "import getpass\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "from IPython.display import HTML, Audio, display, Javascript\n",
        "from pydub import AudioSegment\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "from groq import Groq\n",
        "from gtts import gTTS\n",
        "\n",
        "print(\"‚úÖ Environment ready.\")"
      ],
      "metadata": {
        "id": "bIVW-Eh_YAfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Audio Input Interface\n",
        "Executes a JavaScript bridge to access the browser's microphone, automatically capturing a 5-second audio sample for processing."
      ],
      "metadata": {
        "id": "wfEZCqtu4-8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AUDIO INPUT INTERFACE (Automatic Capture) ---\n",
        "\n",
        "def record_audio_simple(duration=5, filename='input_audio.wav'):\n",
        "    \"\"\"\n",
        "    Records audio from the browser microphone for a fixed duration\n",
        "    and saves it to the Colab runtime.\n",
        "    \"\"\"\n",
        "    print(f\"üéôÔ∏è Recording for {duration} seconds... Please speak now.\")\n",
        "\n",
        "    js_code = \"\"\"\n",
        "    const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "    const b2text = blob => new Promise(resolve => {\n",
        "      const reader = new FileReader()\n",
        "      reader.onloadend = e => resolve(e.srcElement.result)\n",
        "      reader.readAsDataURL(blob)\n",
        "    })\n",
        "    var record = time => new Promise(async resolve => {\n",
        "      stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "      recorder = new MediaRecorder(stream)\n",
        "      chunks = []\n",
        "      recorder.ondataavailable = e => chunks.push(e.data)\n",
        "      recorder.start()\n",
        "      await sleep(time)\n",
        "      recorder.onstop = async ()=>{\n",
        "        blob = new Blob(chunks)\n",
        "        text = await b2text(blob)\n",
        "        resolve(text)\n",
        "      }\n",
        "      recorder.stop()\n",
        "    })\n",
        "    \"\"\"\n",
        "    display(Javascript(js_code))\n",
        "    s = output.eval_js('record(%d)' % (duration*1000))\n",
        "    binary = b64decode(s.split(',')[1])\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "\n",
        "    return filename\n",
        "\n",
        "# --- EXECUTION ---\n",
        "try:\n",
        "    audio_path = record_audio_simple(duration=5)\n",
        "    print(f\"‚úÖ Audio captured successfully: {audio_path}\")\n",
        "\n",
        "    # Playback for verification\n",
        "    print(\"‚ñ∂Ô∏è Playback:\")\n",
        "    display(Audio(audio_path))\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error recording audio: {str(e)}\")"
      ],
      "metadata": {
        "id": "qg8almpU5oA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. AI Pipeline Execution\n",
        "Orchestrating the flow: Audio Transcription (ASR) $\\to$ Intelligence (LLM) $\\to$ Synthesis (TTS)."
      ],
      "metadata": {
        "id": "F8A7BhH-5ckd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AI PROCESSING PIPELINE ---\n",
        "\n",
        "def run_pipeline(audio_file):\n",
        "    # 1. ASR: Speech-to-Text\n",
        "    print(\"\\n1Ô∏è‚É£ Transcribing audio (Whisper)...\")\n",
        "    model = whisper.load_model(\"base\")\n",
        "    transcription = model.transcribe(audio_file)[\"text\"]\n",
        "    print(f\"   ‚îî‚îÄ‚îÄ User Query: \\\"{transcription.strip()}\\\"\")\n",
        "\n",
        "    # 2. LLM: Inference\n",
        "    print(\"\\n2Ô∏è‚É£ Generating response (Llama 3.3)...\")\n",
        "    try:\n",
        "        if 'client' not in locals(): # API Key check\n",
        "            print(\"   üîë Enter Groq API Key:\")\n",
        "            api_key = getpass.getpass()\n",
        "            client = Groq(api_key=api_key)\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Eres un asistente de IA conciso y profesional. Responde en espa√±ol.\"},\n",
        "                {\"role\": \"user\", \"content\": transcription}\n",
        "            ],\n",
        "            temperature=0.5,\n",
        "            max_tokens=200\n",
        "        )\n",
        "        ai_response = chat_completion.choices[0].message.content\n",
        "        print(f\"   ‚îî‚îÄ‚îÄ AI Response: \\\"{ai_response[:100]}...\\\"\") # Preview output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in LLM inference: {str(e)}\", None\n",
        "\n",
        "    # 3. TTS: Audio Synthesis\n",
        "    print(\"\\n3Ô∏è‚É£ Synthesizing audio (gTTS)...\")\n",
        "    tts = gTTS(text=ai_response, lang='es')\n",
        "    output_path = \"ai_response.wav\"\n",
        "    tts.save(output_path)\n",
        "\n",
        "    return ai_response, output_path\n",
        "\n",
        "# --- RUN PIPELINE ---\n",
        "# Execute the full flow using the captured audio\n",
        "if os.path.exists(audio_path):\n",
        "    response_text, response_audio = run_pipeline(audio_path)\n",
        "\n",
        "    print(\"\\n‚úÖ Interaction Complete.\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"ü§ñ Full Response:\\n{response_text}\")\n",
        "    print(\"=\"*50)\n",
        "    display(Audio(response_audio, autoplay=True))\n",
        "else:\n",
        "    print(\"‚ùå No audio input found. Please run the recording cell first.\")"
      ],
      "metadata": {
        "id": "6PjEzBFW6CZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Technical Notes & Future Improvements\n",
        "\n",
        "### Performance Analysis\n",
        "* **Latency:** The Whisper `base` model provides a good trade-off between speed and accuracy for this demo. For production, `distil-whisper` could reduce ASR latency by 50%.\n",
        "* **Inference:** Using Groq's LPU (Language Processing Unit) ensures token generation speeds significantly faster than standard GPU inference.\n",
        "\n",
        "### Stack References\n",
        "* **Whisper:** [OpenAI GitHub](https://github.com/openai/whisper)\n",
        "* **Llama 3:** [Meta AI](https://llama.meta.com/)\n",
        "* **Groq Cloud:** [Groq API Docs](https://console.groq.com/docs/quickstart)"
      ],
      "metadata": {
        "id": "PY8c13_F7PGv"
      }
    }
  ]
}